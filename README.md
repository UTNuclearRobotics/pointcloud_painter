# Table of Contents
1. [About](#about)
2. [Parameter Setup](#parameter-setup)
3. [Usage](#usage)

*** 

## About
This ROS package projects RGB camera imagery onto 3D depth pointclouds of scenes which are generated by separate sensors (such as regular cameras and LIDAR units), "painting" the clouds into RGBXYZ clouds. This can be done for various image formats - so far, flat 2D rasters and spherical camera imagery are implemented, with both equisolid and equidistant projections implemented for the latter.

<img src=images/painted_cloud.png width="600">
<img src=images/panorama.png width="600">

The above images illustrate a painted pointcloud of an example scene, with a panoramic camera image of the same scene displayed below for comparison. The panorama was taken a few days after the pointcloud was painted, so a few temporary objects have moved in the latter scene. 

### Basics of Approach
First, the RGB raster image is converted into a pointcloud version and projected onto the unit sphere about the target frame. A copy of the depth cloud is created with points projected onto this same sphere. For every point in the depth cloud a nearest neighbor search is performed on the RGB cloud and an inverse-distance-weighted assignment of color between a user-specified k number of neighbors is performed. The colors are assigned to the unprojected depth cloud to yield an RGBXYZ cloud. Various downsampling options are available at multiple points within this process to improve speed. 

### Advantages Over Stereo Vision
Commercial stereo vision and structure-from-motion sensors allow simultaneous estimation of RGB and XYZ data for an environment. However, these approaches depend on estimation of feature correspondences between images, and perform poorly for surfaces which are smooth, untextured, and highly self-similar. They do not have a direct basis on empirical measurement of distance, unlike time-of-flight approaches like LIDAR. This package was originally designed specifically to generate highly accurate and precise RGBXYZ images of scenes which would cause difficulty for stereo vision, such as large, regular, flat concrete facades. This approach performs very well for such scenes, where stereo vision would struggle. 

### Limitations and Future Work
This kind of approach lends itself best to large surfaces being painted in uncluttered scenes, where there is not much complexity of fore- vs background. Differences in occlusion of further objects by closer ones from the respective perspectives of the depth and RGB sensors can lead to problems for very cluttered scenes. These errors are reduced for smaller offsets between the sensors and for objects that are more distant. We are aiming to develop a sensor tree which places these two sensors essentially on top of one another, which should largely eliminate this problem.

## Parameter Setup

## Usage
